import { GoogleGenerativeAI } from "@google/generative-ai";
import { Difficulty, GeneratedMCQResponse, MentorResponse } from "../types";

// --- L·∫•y API Key ---
const apiKey = import.meta.env.VITE_GEMINI_API_KEY || '';

// Kh·ªüi t·∫°o Client
const genAI = new GoogleGenerativeAI(apiKey);

// --- C·∫§U H√åNH MODEL (D√πng t√™n m√£ an to√†n nh·∫•t) ---
// S·ª≠ d·ª•ng h·∫≠u t·ªë '-latest' ƒë·ªÉ t·ª± ƒë·ªông ch·ªçn b·∫£n ph√π h·ª£p nh·∫•t v·ªõi Key c·ªßa b·∫°n
const MODEL_NAME = "gemini-1.5-flash-latest";

const generationConfig = {
  temperature: 1,
  topP: 0.95,
  topK: 64,
  maxOutputTokens: 8192,
  responseMimeType: "application/json",
};

interface ContentFile {
    content: string;
    isText: boolean;
}

const LIMIT_THEORY_CHARS = 200000; 
const LIMIT_CLINICAL_CHARS = 100000; 
const LIMIT_SAMPLE_CHARS = 50000; 

// --- H√ÄM RETRY ---
const wait = (ms: number) => new Promise(resolve => setTimeout(resolve, ms));

async function retryGeminiCall<T>(call: () => Promise<T>, retries = 3, delay = 2000): Promise<T> {
  try {
    return await call();
  } catch (error: any) {
    // N·∫øu l·ªói 404 (Model not found) ho·∫∑c 429/503 (Qu√° t·∫£i) -> Th·ª≠ l·∫°i
    const msg = error.message || "";
    if (retries > 0 && (msg.includes("429") || msg.includes("503") || msg.includes("404"))) {
      console.warn(`Gemini Error (${msg}). Retrying in ${delay}ms...`);
      await wait(delay);
      return retryGeminiCall(call, retries - 1, delay * 2);
    }
    throw error;
  }
}

export const generateMCQQuestions = async (
  topic: string,
  count: number,
  difficulties: Difficulty[],
  files: { theory?: ContentFile[]; clinical?: ContentFile[]; sample?: ContentFile[] } = {}
): Promise<GeneratedMCQResponse> => {
  if (!apiKey) throw new Error("API Key is missing");

  const model = genAI.getGenerativeModel({ 
    model: MODEL_NAME,
    systemInstruction: `B·∫°n l√† gi√°o s∆∞ Y khoa. T·∫°o ${count} c√¢u h·ªèi tr·∫Øc nghi·ªám gi·∫£i ph·∫´u ch·ªß ƒë·ªÅ "${topic}". ƒê·ªô kh√≥: ${difficulties.join(', ')}. Output JSON: { "questions": [...] }`
  });

  const parts: any[] = [];
  const addFiles = (list: ContentFile[] | undefined, label: string, limit: number) => {
    if (!list) return;
    let chars = 0;
    parts.push({ text: `\n=== ${label} ===\n` });
    for (const f of list) {
        if (chars >= limit) break;
        if (f.isText) {
            parts.push({ text: f.content.substring(0, limit - chars) });
            chars += f.content.length;
        } else {
            const base64 = f.content.includes('base64,') ? f.content.split('base64,')[1] : f.content;
            parts.push({ inlineData: { mimeType: "application/pdf", data: base64 }});
            chars += 10000;
        }
    }
  };

  addFiles(files.theory, "L√ù THUY·∫æT", LIMIT_THEORY_CHARS);
  addFiles(files.clinical, "L√ÇM S√ÄNG", LIMIT_CLINICAL_CHARS);
  addFiles(files.sample, "ƒê·ªÄ M·∫™U", LIMIT_SAMPLE_CHARS);
  parts.push({ text: `T·∫°o ${count} c√¢u h·ªèi JSON.` });

  try {
    const result = await retryGeminiCall(() => model.generateContent({
        contents: [{ role: 'user', parts }],
        generationConfig
    }));
    const text = result.response.text().replace(/```json|```/g, '').trim();
    return JSON.parse(text);
  } catch (e: any) {
    throw new Error("L·ªói AI: " + e.message);
  }
};

export const generateStationQuestionFromImage = async (base64Image: string, topic?: string): Promise<any> => {
    const model = genAI.getGenerativeModel({ 
        model: MODEL_NAME, 
        generationConfig: { responseMimeType: "application/json" }
    });
    const cleanBase64 = base64Image.includes('base64,') ? base64Image.split('base64,')[1] : base64Image;
    try {
        const result = await retryGeminiCall(() => model.generateContent([
            topic ? `Ch·ªß ƒë·ªÅ ${topic}. T·∫°o c√¢u h·ªèi tr·∫°m.` : "T·∫°o c√¢u h·ªèi tr·∫°m.",
            { inlineData: { mimeType: "image/jpeg", data: cleanBase64 } }
        ]));
        return JSON.parse(result.response.text().replace(/```json|```/g, '').trim());
    } catch (e) { return { isValid: false, questions: [] }; }
};

export const analyzeResultWithOtter = async (topic: string, stats: any): Promise<MentorResponse> => {
    const model = genAI.getGenerativeModel({ 
        model: MODEL_NAME, 
        generationConfig: { responseMimeType: "application/json" }
    });
    try {
        const result = await retryGeminiCall(() => model.generateContent(
            `Ph√¢n t√≠ch k·∫øt qu·∫£ thi ${topic}: ${JSON.stringify(stats)}. Output JSON mentor.`
        ));
        return JSON.parse(result.response.text().replace(/```json|```/g, '').trim());
    } catch (e) { return { analysis: "L·ªói...", strengths: [], weaknesses: [], roadmap: [] }; }
};

export const chatWithOtter = async (history: any[], message: string, image?: string): Promise<string> => {
    const model = genAI.getGenerativeModel({ model: MODEL_NAME });
    const chat = model.startChat({
        history: history.map(h => ({ role: h.role === 'model' ? 'model' : 'user', parts: [{ text: h.text }] }))
    });
    try {
        let result;
        if (image) {
             const cleanBase64 = image.includes('base64,') ? image.split('base64,')[1] : image;
             result = await model.generateContent([message, { inlineData: { mimeType: "image/jpeg", data: cleanBase64 } }]);
        } else {
             result = await chat.sendMessage(message);
        }
        return result.response.text();
    } catch (e) { return "R√°i c√° ƒëang b·∫≠n... ü¶¶"; }
};
